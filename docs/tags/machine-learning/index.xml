<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on thomjiji</title>
    <link>http://localhost:1313/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on thomjiji</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 07 Sep 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Machine Learning Glossary</title>
      <link>http://localhost:1313/blogs/2023/09/machine-learning-glossary/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/2023/09/machine-learning-glossary/</guid>
      <description>&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/glossary&#34; target=&#34;_blank&#34; &gt;Machine Learning Glossary - Google for
Developers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;activation-function&#34;&gt;activation function&lt;/h2&gt;
&lt;p&gt;A function that enables neural networks to learn non-linear (complex) relationships
between features and the label. Popupar activation functions include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReLU&lt;/li&gt;
&lt;li&gt;Sigmoid&lt;/li&gt;
&lt;li&gt;Tanh&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;backpropagation&#34;&gt;backpropagation&lt;/h2&gt;
&lt;p&gt;The algorithm that implements gradient descent in neural networks.&lt;/p&gt;
&lt;p&gt;Training a neural network involves many iterations of the following two-pass cycle:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;During the &lt;strong&gt;forward pass&lt;/strong&gt;, the system processes a batch of examples to yild
prediction(s). The system compares each prediction to each label value. The
difference between the prediction and the lable value is the loss for that example.
The system aggregates the losses for all the examples to compute the total loss for
the current batch.&lt;/li&gt;
&lt;li&gt;During the &lt;strong&gt;backward pass&lt;/strong&gt; (backpropagation), the system reduces loss by adjusting
the weights of all the neurons in all the hidden layer(s).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Neural networks often contain many neurons across many hidden layers. Each of those
neurons contribute to the overall loss in difference ways. Backpropagation determines
whether to increase or decrease the weights applied to particular neurons.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
