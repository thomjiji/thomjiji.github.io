<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Docker on thomjiji</title>
    <link>https://thomjiji.github.io/tags/docker/</link>
    <description>Recent content in Docker on thomjiji</description>
    <generator>Hugo -- 0.146.7</generator>
    <language>en-US</language>
    <lastBuildDate>Fri, 02 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://thomjiji.github.io/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tailscale and Local LLM</title>
      <link>https://thomjiji.github.io/blogs/2025/05/tailscale-and-local-llm/</link>
      <pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate>
      <guid>https://thomjiji.github.io/blogs/2025/05/tailscale-and-local-llm/</guid>
      <description>&lt;h2 id=&#34;tailscaleopen-webui-和-ollama--llamacpp-整体结构&#34;&gt;Tailscale、Open WebUI 和 Ollama / llama.cpp 整体结构&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;目标：让我的 MacBook Pro（MBP）使用本地大模型，但避免性能负担&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我的 M1 Max 32G MacBook Pro 本身性能有限，而且我不希望在使用它的时候，风扇不停地转，大腿慢慢感到它在升温红温起来，运行大模型时显得吃力。因此，我把实际的模型运行和推理的任务交给远程的两台机器：gsj-13 和 gsj-9（根据负载情况选择使用哪一台）。&lt;/p&gt;
&lt;p&gt;我通过 Tailscale 将这些设备连接到了一个统一的 Tailnet 中，这样它们就处于同一个虚拟私有网络（VPN）下。通过 Tailscale 的 ACL (Access Control Lists) 规则，只有我可以访问它们，它们无法访问我。&lt;/p&gt;
&lt;p&gt;目前，Open WebUI 是运行在 gsj-13 的 Docker 容器中。它支持连接到本地或远程的模型服务。如果我选择在 gsj-13 本地运行 Ollama，那么 Open WebUI 的后端地址就是 &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;；如果我选择将模型推理任务交给性能更强、内存更大的 gsj-9，那么后端地址就设置为 gsj-9 的局域网 IP 地址：&lt;code&gt;http://&amp;lt;局域网 IP&amp;gt;:11434&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;我用 MBP 打开浏览器，输入 gsj-13 在 Tailnet 的 IP 地址，接口是 Open WebUI 默认的 3000，即可进入 Open WebUI 界面。&lt;/p&gt;
&lt;p&gt;另外，我也在尝试使用 llama.cpp，因为它提供了对模型运行参数更细粒度的控制，我能更详细了解模型推理的时候究竟发生了什么。llama.cpp 中的 llama-server 命令支持 OpenAI 兼容的 API，因此在 Open WebUI 中，我可以通过添加一个 OpenAI 连接，将地址设置为 llama-server 的监听地址和端口。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
